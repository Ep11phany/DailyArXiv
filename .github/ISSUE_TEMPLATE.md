---
title: Latest 15 Papers - March 17, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing](http://arxiv.org/abs/2503.10639v1)** | 2025-03-13 | <details><summary>Datas...</summary><p>Dataset and models are released in https://github.com/rongyaofang/GoT</p></details> |
| **[HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](http://arxiv.org/abs/2503.10631v1)** | 2025-03-13 |  |
| **[UniGoal: Towards Universal Zero-shot Goal-oriented Navigation](http://arxiv.org/abs/2503.10630v1)** | 2025-03-13 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |
| **[Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](http://arxiv.org/abs/2503.10619v1)** | 2025-03-13 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025 Trustworthy LLM</p></details> |
| **[From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM](http://arxiv.org/abs/2503.10620v1)** | 2025-03-13 |  |
| **[Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models](http://arxiv.org/abs/2503.10617v1)** | 2025-03-13 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025 SCOPE</p></details> |
| **[R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization](http://arxiv.org/abs/2503.10615v1)** | 2025-03-13 | <details><summary>Code ...</summary><p>Code and Model: https://github.com/Fancy-MLLM/R1-onevision</p></details> |
| **[CoSTA$\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing](http://arxiv.org/abs/2503.10613v1)** | 2025-03-13 |  |
| **[TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention](http://arxiv.org/abs/2503.10602v1)** | 2025-03-13 | <details><summary>15 pa...</summary><p>15 pages, 9 figures, the first two authors contributed equally</p></details> |
| **[Unlock the Power of Unlabeled Data in Language Driving Model](http://arxiv.org/abs/2503.10586v1)** | 2025-03-13 | Accepted by ICRA2025 |
| **[Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative Study of Large Language Models](http://arxiv.org/abs/2503.10573v1)** | 2025-03-13 |  |
| **[ASIDE: Architectural Separation of Instructions and Data in Language Models](http://arxiv.org/abs/2503.10566v1)** | 2025-03-13 | <details><summary>ICLR ...</summary><p>ICLR 2025 Workshop on Building Trust in Language Models and Applications</p></details> |
| **[BIMBA: Selective-Scan Compression for Long-Range Video Question Answering](http://arxiv.org/abs/2503.09590v2)** | 2025-03-13 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[Short-term AI literacy intervention does not reduce over-reliance on incorrect ChatGPT recommendations](http://arxiv.org/abs/2503.10556v1)** | 2025-03-13 |  |
| **[KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation](http://arxiv.org/abs/2503.10546v1)** | 2025-03-13 | <details><summary>Proje...</summary><p>Project website: http://kuda-dynamics.github.io</p></details> |

## RAG
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Balancing Content Size in RAG-Text2SQL System](http://arxiv.org/abs/2502.15723v2)** | 2025-03-12 |  |
| **[RAG-Adapter: A Plug-and-Play RAG-enhanced Framework for Long Video Understanding](http://arxiv.org/abs/2503.08576v1)** | 2025-03-11 | 37 pages, 36 figures |
| **[RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration](http://arxiv.org/abs/2503.02800v3)** | 2025-03-11 | <details><summary>arXiv...</summary><p>arXiv admin note: substantial text overlap with arXiv:2411.00914</p></details> |
| **[OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning](http://arxiv.org/abs/2503.08398v1)** | 2025-03-11 |  |
| **[OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation](http://arxiv.org/abs/2412.02592v2)** | 2025-03-11 | Work in progress |
| **[Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents](http://arxiv.org/abs/2406.05870v4)** | 2025-03-10 | <details><summary>To ap...</summary><p>To appear in USENIX Security Symposium 2025</p></details> |
| **[RAG-Enhanced Collaborative LLM Agents for Drug Discovery](http://arxiv.org/abs/2502.17506v2)** | 2025-03-10 | <details><summary>Machi...</summary><p>Machine Learning, Drug Discovery</p></details> |
| **[SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback](http://arxiv.org/abs/2410.18141v2)** | 2025-03-10 |  |
| **[Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving](http://arxiv.org/abs/2503.06567v1)** | 2025-03-09 |  |
| **[MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks](http://arxiv.org/abs/2502.17832v2)** | 2025-03-09 | <details><summary>Code ...</summary><p>Code is available at https://github.com/HyeonjeongHa/MM-PoisonRAG</p></details> |
| **[FinTMMBench: Benchmarking Temporal-Aware Multi-Modal RAG in Finance](http://arxiv.org/abs/2503.05185v1)** | 2025-03-07 | Under review |
| **[Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge Reasoning](http://arxiv.org/abs/2503.04973v1)** | 2025-03-06 |  |
| **[More Documents, Same Length: Isolating the Challenge of Multiple Documents in RAG](http://arxiv.org/abs/2503.04388v1)** | 2025-03-06 | Preprint |
| **[In-depth Analysis of Graph-based RAG in a Unified Framework](http://arxiv.org/abs/2503.04338v1)** | 2025-03-06 |  |
| **[LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs -- No Silver Bullet for LC or RAG Routing](http://arxiv.org/abs/2502.09977v2)** | 2025-03-05 | 22 pages |

